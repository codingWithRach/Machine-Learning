{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Chapter 18\n",
    "# Estimation with Cross-Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Cross-validation is a resampling procedure used to evaluate the skill of machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.  As such, the procedure is often called k-fold cross-validation.\n",
    "\n",
    "It is a popular method because it is simple to understand, and generally results in a less biased or less optimistic estimate of the model skill than other methods (such as a simple train/test split)\n",
    "\n",
    "1. shuffle the dataset randomly\n",
    "2. split the dataset into k groups of approximately equal size\n",
    "3. for each unique group\n",
    "- take the group as a hold out or test data set\n",
    "- take the remaining groups as a training data set\n",
    "- fit a model on the training set\n",
    "- evaluate the model on the test set\n",
    "- retain the evaluation score and discard the model\n",
    "4. summarise the skill of the model using the sample of model evauation scores\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group, and stays in that group for the duration of the procedure.  Thus each sample has the opportunity to be used in the test set once, and in the training set (k-1) times\n",
    "\n",
    "It is important that any preparation of the data prior to fitting the model occur on the cross-validation-assigned training dataset within the loop, rather than on the broader dataset.  This prevents data leakage and an optimistic estimate of the model skill.\n",
    "\n",
    "The results of a k-fold cross-validation run are often summarised with the mean of the model skill scores.  It is also good practice to include a measure of the variance, such as the standard deivation or standard error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Configuration of k\n",
    "The k-value must be chosen carefully for your data sample.  A poorly chosen value may result in a mis-representative idea of the skill of the model (e.g. high variance), or a high bias (e.g. overestimate of skill).  Common tactics for choosing k:\n",
    "- Representative - k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset\n",
    "- k=10 - this value has been found through experimentation to generally result in a model skill estiamte with low bias and a modest variance\n",
    "- k=n - this gives each test sample an opportunity to be used in the test set.  This approach is called leave-one-out cross-validation\n",
    "\n",
    "The choice of k is usually 5 or 10, but there is no formal rule.  As k gets larger, the difference in size between the training set and resampling subsets gets smaller, and the bias of the technique reduces.\n",
    "\n",
    "If you are struggling to choose a value, k=10 is recommended.  However, it is preferable to split the data sample into k groups with the same number of samples if possible, such that the sample of model skill scores are all equivalent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Worked Example\n",
    "Imagine we have a data set with 6 observations:\n",
    "- [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "Pick a value for k - we will choose k=3.  Shuffle the data and then split into 3 groups (each will have an equal number of 2 observations):\n",
    "- Fold1: [0.5, 0.2]\n",
    "- Fold2: [0.1, 0.3]\n",
    "- Fold3: [0.4, 0.6]\n",
    "\n",
    "Use the sample to evaluate the skill of a machine learning algorithm.  Three models are trainiend and evaluated, with each fold given a chance to be the test set:\n",
    "- Model1: trained on Fold1 + Fold2, Tested on Fold3\n",
    "- Model2: trained on Fold2 + Fold3, Tested on Fold1\n",
    "- Model3: trained on Fold1 + Fold3, Tested on Fold2\n",
    "\n",
    "The models are discarded after they are evaluated.  The skill scores are collected for each model and summarised for use.\n",
    "\n",
    "We do not need to implement k-fold cross-validation manually.  The scikit-learn library provides the KFold() class to split a given data sample.  It takes as arguments:\n",
    "- the number of splits\n",
    "- whether or not to shuffle the sample\n",
    "- the seed for the pseudorandom number generator used prior to the shuffle\n",
    "\n",
    "The split() function can then be called repeatedly on the class, where the data sample is provided as an argument.  Arrays are returned containing the indices into the original data sample of observations to use for train and test sets on each iteration.\n",
    "\n",
    "Usually the k-fold cross-validation implementation in scikit-learn is provided as a component operation within broader methods, such as grid-searching model hyperparameters and scoring a model on a dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\ntrain: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\ntrain: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn k-fold cross-validation\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# data sample\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "\n",
    "# prepare cross validation to split the dataset into 3 folds, shuffling prior to the split, with a seed of 1\n",
    "kfold = KFold(3, True, 1)\n",
    "\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "source": [
    "# Variations on Cross-Validation\n",
    "Commonly used variations on the k-fold cross-validation procedure are:\n",
    "- Train/Test Split - taken to one extreme, k may be set to 1\n",
    "- LOOCV - taken to another extreme, k may be set to the total number of observations in the dataset, so that each observation is given a chance to be the test dataset.  This is called leave-one-out cross-validation (LOOCV)\n",
    "- Stratified - the splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value\n",
    "- Repeated - the k-fold cross-validation procedure is repeated n times where, importantly, the data sample is shuffled prior to each repetition, thus resulting in a different split of the sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Extensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data: [35 27 78 64 18 24 82 87 60 37 95 65 68 61 18 42 92 85 24 73 33 45 10 38\n 76 78 88 47 23 56 69 42 12 85 64 13  5 43  7  7 84 13 38 54 77 54 69 61\n 50  6 29 78 96 90  1 57 38 74 36 59 37  0 91  2 20 84 63 87 41 14 88  4\n 96 16 56 61 92 55 29  0 13 70  4 56 44  1 81 62 34 65 91 91 22 39 99 81\n 20 13 59 59]\ntrain: [35 27 78 64 18 24 82 87 60 37 95 65 68 61 18 42 92 24 73 33 45 10 38 76\n 78 88 47 23 56 69 42 12 64 13 43  7  7 84 13 38 54 77 54 69 61 50  6 29\n 78 96 90  1 57 38 74 36 59 37  0 91  2 20 63 87 41 88  4 96 16 56 61 92\n 55 29  0 56  1 81 62 34 65 91 91 22 99 81 20 13 59 59], test: [85 85  5 84 14 13 70  4 44 39]\ntrain: [35 27 78 64 18 24 82 87 60 37 65 68 61 18 42 92 85 24 73 33 45 10 38 76\n 78 88 47 23 56 69 85 64 13  5 43  7 84 13 38 54 54 69 61 50  6 29 90  1\n 57 74 36 59 37  0 91  2 20 84 63 87 41 14 88  4 96 16 56 61 92 55  0 13\n 70  4 56 44  1 81 62 34 65 91 91 39 99 81 20 13 59 59], test: [95 42 12  7 77 78 96 38 29 22]\ntrain: [35 27 64 18 24 82 87 60 37 95 65 68 61 18 42 92 85 24 33 45 10 38 76 78\n 88 23 56 69 42 12 85 64  5 43  7 84 13 38 54 77 54 61 50  6 29 78 96 90\n  1 57 38 74 36 59 37  0  2 20 84 63 87 41 14 88  4 96 56 61 92 55 29  0\n 13 70  4 56 44  1 81 62 34 65 91 91 22 39 81 20 59 59], test: [78 73 47 13  7 69 91 16 99 13]\ntrain: [35 27 78 64 18 24 82 87 60 37 95 65 68 61 18 42 92 85 24 73 33 45 10 76\n 78 88 47 23 56 69 42 12 85 13  5 43  7  7 13 38 54 77 54 69 61  6 29 78\n 96 90 57 38 74 36 37  0 91  2 20 84 63 41 14 88  4 96 16 56 61 92 55 29\n  0 13 70  4 56 44  1 81 62 65 91 91 22 39 99 20 13 59], test: [38 64 84 50  1 59 87 34 81 59]\ntrain: [35 27 78 64 18 24 82 87 60 37 95 65 68 61 18 92 85 24 73 33 45 10 38 76\n 78 47 23 56 69 42 12 85 64 13  5 43  7  7 84 38 54 77 69 61 50  6 29 78\n 96  1 57 38 74 36 59 37  0 91  2 20 84 63 87 41 14 88  4 96 16 56 61 92\n 29  0 13 70  4 44  1 62 34 65 91 22 39 99 81 20 13 59], test: [42 88 13 54 90 55 56 81 91 59]\ntrain: [35 27 78 24 82 87 60 37 95 65 68 61 18 42 92 85 24 73 33 10 38 78 88 47\n 23 56 69 42 12 85 64 13  5 43  7  7 84 13 38 77 54 69 61 50 29 78 96 90\n  1 38 74 59 37  0 91  2 20 84 63 87 41 14 88  4 96 16 61 92 55 29  0 13\n 70  4 56 44  1 81 34 65 91 91 22 39 99 81 20 13 59 59], test: [64 18 45 76 54  6 57 36 56 62]\ntrain: [27 78 64 18 24 82 87 37 95 65 68 61 18 42 92 85 24 73 33 45 10 38 76 78\n 88 47 23 56 42 12 85 64 13  5 43  7  7 84 13 54 77 54 69 50  6 29 78 96\n 90  1 57 38 74 36 59  0 91  2 20 84 87 41 14  4 96 16 56 61 92 55 29  0\n 13 70  4 56 44  1 81 62 34 91 22 39 99 81 20 13 59 59], test: [35 60 69 38 61 37 63 88 65 91]\ntrain: [35 27 78 64 18 24 82 60 37 95 65 68 42 92 85 24 73 33 45 38 76 78 88 47\n 23 56 69 42 12 85 64 13  5 43  7  7 84 13 38 54 77 54 69 61 50  6 29 78\n 96 90  1 57 38 36 59 37 91 20 84 63 87 14 88  4 96 16 56 61 92 55 29  0\n 13 70  4 56 44 81 62 34 65 91 91 22 39 99 81 13 59 59], test: [87 61 18 10 74  0  2 41  1 20]\ntrain: [35 27 78 64 18 24 87 60 37 95 68 61 18 42 92 85 73 45 10 38 76 88 47 69\n 42 12 85 64 13  5 43  7  7 84 13 38 54 77 54 69 61 50  6 78 96 90  1 57\n 38 74 36 59 37  0 91  2 20 84 63 87 41 14 88 96 16 56 61 55 29  0 13 70\n  4 56 44  1 81 62 34 65 91 91 22 39 99 81 20 13 59 59], test: [82 65 24 33 78 23 56 29  4 92]\ntrain: [35 78 64 18 82 87 60 95 65 61 18 42 85 24 73 33 45 10 38 76 78 88 47 23\n 56 69 42 12 85 64 13  5  7  7 84 13 38 54 77 54 69 61 50  6 29 78 96 90\n  1 57 38 74 36 59 37  0 91  2 84 63 87 41 14 88  4 16 56 92 55 29 13 70\n  4 56 44  1 81 62 34 65 91 91 22 39 99 81 20 13 59 59], test: [27 24 37 68 92 43 20 96 61  0]\n"
     ]
    }
   ],
   "source": [
    "# write your own function to split a data sample using k-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# unseeded data sample\n",
    "data = np.random.randint(0, 100, 100)\n",
    "print('Data:', data)\n",
    "\n",
    "# prepare cross validation to split the dataset into 10 folds, shuffling prior to the split, with a seed of 1\n",
    "kfold = KFold(10, True, 1)\n",
    "\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "source": [
    "develop examples to demonstrate each of the main types of cross-validation supported by scikit-learn\n",
    "- GroupKFold\n",
    "- GroupShuffleSplit\n",
    "- LeaveOneGroupOut\n",
    "- LeavePGroupsOut\n",
    "- LeaveOneOut\n",
    "- LeavePOut\n",
    "- PredefinedSplit\n",
    "- RepeatedKFold\n",
    "- RepeatedStratifiedKFold\n",
    "- ShuffleSplit\n",
    "- StratifiedKFold\n",
    "- StratifiedShuffleSplit\n",
    "- TimeSeriesSplit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [0 1]\nX_train [[1 2]\n [3 4]]\ny train [10 20]\nTEST INDEX: [2 3]\nX test [[5 6]\n [7 8]]\ny test [30 40] \n\nTRAIN INDEX: [2 3]\nX_train [[5 6]\n [7 8]]\ny train [30 40]\nTEST INDEX: [0 1]\nX test [[1 2]\n [3 4]]\ny test [10 20] \n\n"
     ]
    }
   ],
   "source": [
    "# GroupKFold example\n",
    "# k-fold iterator variant with non-overlapping groups - the same group will not appear in two different folds.  The number of distinct groups must be at least equal to the number of folds, and the number of distinct groups is approximately the same in each fold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([10, 20, 30, 40])\n",
    "groups = np.array([0, 0, 2, 2])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(X, y, groups)\n",
    "\n",
    "GroupKFold(n_splits=2)\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X_train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8,)\nTRAIN INDEX: [2 3 4 5 6 7]\nTEST INDEX: [0 1]\nTRAIN INDEX: [0 1 5 6 7]\nTEST INDEX: [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# GroupShuffleSplit example\n",
    "# provides randomised train/test indices to split data according to a third-party provided group.  This group information can be used to encode arbitrary domain specific stratifications of the samples as integers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = np.ones(shape=(8, 2))\n",
    "y = np.ones(shape=(8, 1))\n",
    "groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
    "print(groups.shape)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
    "gss.get_n_splits()\n",
    "\n",
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "    print(\"TRAIN INDEX:\", train_idx)\n",
    "    print(\"TEST INDEX:\", test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [2 3]\nX train [[5 6]\n [7 8]]\ny train [11 22]\nTEST INDEX: [0 1]\nX test [[1 2]\n [3 4]]\ny test [10 20] \n\nTRAIN INDEX: [0 1]\nX train [[1 2]\n [3 4]]\ny train [10 20]\nTEST INDEX: [2 3]\nX test [[5 6]\n [7 8]]\ny test [11 22] \n\n"
     ]
    }
   ],
   "source": [
    "# LeaveOneGroupOut example\n",
    "# provides test/train indices to split data according to a third-party provided group.  This group information can be used to encode arbitrary domain specific stratifications of samples as integers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([10, 20, 11, 22])\n",
    "groups = np.array([1, 1, 2, 2])\n",
    "logo = LeaveOneGroupOut() \n",
    "# logo = LeavePGroupsOut(n_groups = 2) # number of groups to leave out in the test split\n",
    "logo.get_n_splits(X, y, groups)\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')\n",
    "\n",
    "# the difference between LeaveOneGroupOut and LeavePGroupsOut is that:\n",
    "# - the former uses samples all assigned to the same groups\n",
    "# - the latter builds the test sets with all the samples assigned to p different values of the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [1 2]\nX train [[3 4]\n [5 6]]\ny train [20 30]\nTEST INDEX: [0]\nX test [[1 2]]\ny test [10] \n\nTRAIN INDEX: [0 2]\nX train [[1 2]\n [5 6]]\ny train [10 30]\nTEST INDEX: [1]\nX test [[3 4]]\ny test [20] \n\nTRAIN INDEX: [0 1]\nX train [[1 2]\n [3 4]]\ny train [10 20]\nTEST INDEX: [2]\nX test [[5 6]]\ny test [30] \n\n"
     ]
    }
   ],
   "source": [
    "# LeaveOneOut example\n",
    "# provides train/test indices to split data in train/test sets.  Each sample is used once as a test set (singleton) while the remaining samples form the training set\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([10, 20, 30])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [2 3]\nX train [[5 6]\n [7 8]]\ny train [3 4]\nTEST INDEX: [0 1]\nX test [[1 2]\n [3 4]]\ny test [1 2] \n\nTRAIN INDEX: [1 3]\nX train [[3 4]\n [7 8]]\ny train [2 4]\nTEST INDEX: [0 2]\nX test [[1 2]\n [5 6]]\ny test [1 3] \n\nTRAIN INDEX: [1 2]\nX train [[3 4]\n [5 6]]\ny train [2 3]\nTEST INDEX: [0 3]\nX test [[1 2]\n [7 8]]\ny test [1 4] \n\nTRAIN INDEX: [0 3]\nX train [[1 2]\n [7 8]]\ny train [1 4]\nTEST INDEX: [1 2]\nX test [[3 4]\n [5 6]]\ny test [2 3] \n\nTRAIN INDEX: [0 2]\nX train [[1 2]\n [5 6]]\ny train [1 3]\nTEST INDEX: [1 3]\nX test [[3 4]\n [7 8]]\ny test [2 4] \n\nTRAIN INDEX: [0 1]\nX train [[1 2]\n [3 4]]\ny train [1 2]\nTEST INDEX: [2 3]\nX test [[5 6]\n [7 8]]\ny test [3 4] \n\n"
     ]
    }
   ],
   "source": [
    "# LeavePOut example\n",
    "# provides train/test indices to split data in train/test sets.  This results in testing on all distinct samples of size p, whilst the remaining (n-p) samples form the training set in each iteration\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "lpo = LeavePOut(2)\n",
    "lpo.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in lpo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')\n",
    "\n",
    "# Note that LeavePOut(p) is NOT equivalent to KFold(n_splits = n_samples // p), which creates non-overlapping test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [1 2 3]\nX train [[3 4]\n [5 6]\n [7 8]]\ny train [20 30 40]\nTEST INDEX: [0]\nX test [[1 2]]\ny test [10] \n\nTRAIN INDEX: [0 2]\nX train [[1 2]\n [5 6]]\ny train [10 30]\nTEST INDEX: [1 3]\nX test [[3 4]\n [7 8]]\ny test [20 40] \n\n"
     ]
    }
   ],
   "source": [
    "# PredefinedSplit example\n",
    "# provides train/test indices to split data into train/test sets using a prefdefined scheme specified by the user with the test_fold parameter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([10, 20, 30, 40])\n",
    "# the entry test_fold[i] represents the index of the test set that sample i belongs to\n",
    "# where test_fold[i] = -1, sample i is included in every training set\n",
    "test_fold = [0, 1, -1, 1]\n",
    "ps = PredefinedSplit(test_fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "for train_index, test_index in ps.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [0 1]\nX train [[1 2]\n [3 4]]\ny train [0 0]\nTEST INDEX: [2 3]\nX test [[1 2]\n [3 4]]\ny test [1 1] \n\nTRAIN INDEX: [2 3]\nX train [[1 2]\n [3 4]]\ny train [1 1]\nTEST INDEX: [0 1]\nX test [[1 2]\n [3 4]]\ny test [0 0] \n\nTRAIN INDEX: [1 2]\nX train [[3 4]\n [1 2]]\ny train [0 1]\nTEST INDEX: [0 3]\nX test [[1 2]\n [3 4]]\ny test [0 1] \n\nTRAIN INDEX: [0 3]\nX train [[1 2]\n [3 4]]\ny train [0 1]\nTEST INDEX: [1 2]\nX test [[3 4]\n [1 2]]\ny test [0 1] \n\n"
     ]
    }
   ],
   "source": [
    "# RepeatedKFold example\n",
    "# repeats k-fold n times, with different randomisation in each repetition\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [1 3 0 4] TEST INDEX: [5 2]\nTRAIN INDEX: [4 0 2 5] TEST INDEX: [1 3]\nTRAIN INDEX: [1 2 4 0] TEST INDEX: [3 5]\nTRAIN INDEX: [3 4 1 0] TEST INDEX: [5 2]\nTRAIN INDEX: [3 5 1 0] TEST INDEX: [2 4]\n\nTRAIN INDEX: [1 3 0] TEST INDEX: [5 2]\nTRAIN INDEX: [4 0 2] TEST INDEX: [1 3]\nTRAIN INDEX: [1 2 4] TEST INDEX: [3 5]\nTRAIN INDEX: [3 4 1] TEST INDEX: [5 2]\nTRAIN INDEX: [3 5 1] TEST INDEX: [2 4]\n"
     ]
    }
   ],
   "source": [
    "# ShuffleSplit example\n",
    "# Yields indices to split data into training and test sets\n",
    "# Randomised CV splitters may return different results for each call of split.  You can make the results identical by setting random_state to an integer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2])\n",
    "\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "rs.get_n_splits(X)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "print('')\n",
    "\n",
    "rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25, random_state=0)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "\n",
    "# Note that contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [1 3] TEST INDEX: [0 2]\nTRAIN INDEX: [0 2] TEST INDEX: [1 3]\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold example\n",
    "# provides train/test indices to split data in train/test data sets.  This cross-validation is a variation of KFold that returns stratified folds.  The folds are made by preserving the percentage of samples for each class.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [5 2 3] TEST INDEX: [4 1 0]\nTRAIN INDEX: [5 1 4] TEST INDEX: [0 2 3]\nTRAIN INDEX: [5 0 2] TEST INDEX: [4 3 1]\nTRAIN INDEX: [4 1 0] TEST INDEX: [2 3 5]\nTRAIN INDEX: [0 5 1] TEST INDEX: [3 4 2]\n"
     ]
    }
   ],
   "source": [
    "# StratifiedShuffleSplit example\n",
    "# provides train/test indices to split data in train/test sets.  This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomised folds.  The folds are made by preserving the percentage of samples for each class.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Note that like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [1 2]\nX train [[3 4]\n [1 2]]\ny train [0 1]\nTEST INDEX: [0 3]\nX test [[1 2]\n [3 4]]\ny test [0 1] \n\nTRAIN INDEX: [0 3]\nX train [[1 2]\n [3 4]]\ny train [0 1]\nTEST INDEX: [1 2]\nX test [[3 4]\n [1 2]]\ny test [0 1] \n\nTRAIN INDEX: [1 3]\nX train [[3 4]\n [3 4]]\ny train [0 1]\nTEST INDEX: [0 2]\nX test [[1 2]\n [1 2]]\ny test [0 1] \n\nTRAIN INDEX: [0 2]\nX train [[1 2]\n [1 2]]\ny train [0 1]\nTEST INDEX: [1 3]\nX test [[3 4]\n [3 4]]\ny test [0 1] \n\n"
     ]
    }
   ],
   "source": [
    "# RepeatedStratifiedKFold example\n",
    "#â–‘Repeats Stratified K-Fold n times, with different randomisation in each repetition\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=36851234)\n",
    "\n",
    "for train_index, test_index in rskf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print('X train', X_train)\n",
    "    print('y train', y_train)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    print('X test', X_test)\n",
    "    print('y test', y_test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN INDEX: [0] TEST INDEX: [1]\nTRAIN INDEX: [0 1] TEST INDEX: [2]\nTRAIN INDEX: [0 1 2] TEST INDEX: [3]\nTRAIN INDEX: [0 1 2 3] TEST INDEX: [4]\nTRAIN INDEX: [0 1 2 3 4] TEST INDEX: [5]\n\nTRAIN INDEX: [0 1 2 3 4 5] TEST INDEX: [6 7]\nTRAIN INDEX: [0 1 2 3 4 5 6 7] TEST INDEX: [8 9]\nTRAIN INDEX: [0 1 2 3 4 5 6 7 8 9] TEST INDEX: [10 11]\n\nTRAIN INDEX: [0 1 2 3] TEST INDEX: [6 7]\nTRAIN INDEX: [0 1 2 3 4 5] TEST INDEX: [8 9]\nTRAIN INDEX: [0 1 2 3 4 5 6 7] TEST INDEX: [10 11]\n"
     ]
    }
   ],
   "source": [
    "# TimeSeriesSplit example\n",
    "# provides train/test indices to split time series data samples that are observed at fixed time inervals, in train/test sets.  In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.\n",
    "# This cross-validation object is a variation of KFold.  In the kth split, it returns first k folds as train set, and the (k+1)th fold as test set\n",
    "# Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "print('')\n",
    "\n",
    "# Fix test_size to 2 with 12 samples\n",
    "X = np.random.randn(12, 2)\n",
    "y = np.random.randint(0, 2, 12)\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "print('')   \n",
    "\n",
    "# Add in a 2 period gap\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "   print(\"TRAIN INDEX:\", train_index, \"TEST INDEX:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}